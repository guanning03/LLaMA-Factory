# ------------------- 基础模型配置 -------------------
model_name_or_path: models/Qwen2.5-3B-Instruct  # 使用Qwen2.5-1.5B基座模型

# ------------------- 训练阶段配置 -------------------
stage: sft
do_train: true
report_to: tensorboard    # Tensorboard设置
logging_dir: ./log_output/qwen3b_instruct_ts_full_llamafactory 
# finetuning_type: lora  # lora微调
# lora_target: all
# lora_rank: 16
flash_attn: fa2


# ------------------- 数据集配置 -------------------
dataset_dir: benchmarks/guanning___math-hard-and-easy_qwen2.5-3_b-instruct_corpus48k_tournament_sft/default/0.0.0/2d1fab251b47eac20668c79ce28af75cfa1d6816
dataset: train_1, train_2              # 对应JSON中定义的数据集名称
max_samples: null  # null表示使用全部数据，如需部分调试可设为具体数值
template: qwen  # 必须使用Qwen对应的模板格式
cutoff_len: 8192  # 上下文最大长度（GSM8K数学题通常不超过此长度）
overwrite_cache: true  # 重新预处理数据时强制刷新缓存
preprocessing_num_workers: 16  # 数据预处理并行进程数（根据CPU核数调整）

# ------------------- 训练输出相关 -------------------
output_dir: ./output/qwen3b_instruct_ts_lora_llamafactory  # 输出目录需要存在可写权限
logging_steps: 10  # 每10步输出一次日志
save_steps: 500  # 每500步保存一次检查点
plot_loss: true  # 绘制训练损失曲线

# ------------------- 训练超参数 -------------------
per_device_train_batch_size: 8  
gradient_accumulation_steps: 1              
learning_rate: 5.0e-6                       # 1.5B模型SFT建议学习率（高于7B但低于large模型）
num_train_epochs: 5                         # GSM8K需更多epoch学习推理逻辑
max_grad_norm: 0.5                          # 梯度裁剪阈值
lr_scheduler_type: cosine                   
warmup_ratio: 0.05                          # warmup阶段占训练总步数的比例
weight_decay: 0.05                          # 新增权重衰减，防止过拟合

# ------------------- 验证与评估 -------------------
val_size: 0.05  # 10%数据作为验证集
per_device_eval_batch_size: 16  # 评估时batch_size可以更大
eval_strategy: steps  # 按步数评估
eval_steps: 500  # 每200步验证一次（GSM8K需要及时评估推理能力）

# ------------------- 显存优化 -------------------
gradient_checkpointing: true  # 激活梯度检查点节省显存
optim: adamw_torch  # 推荐使用AdamW优化器